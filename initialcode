# Arkillikus: Large Language Model AI Chatbot with Retrieval Augmented Generation
# ===================================================================================

import os
import torch
import numpy as np
import pandas as pd
import logging
import time
import pickle
import json
import re
import warnings
from typing import List, Dict, Any, Optional, Tuple, Union, Callable
from pathlib import Path

# NLP/ML imports
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModel,
    pipeline,
    BitsAndBytesConfig
)
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import (
    TextLoader,
    PyPDFLoader,
    CSVLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredMarkdownLoader
)
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("Arkillikus")

# Suppress specific warnings
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings("ignore", category=UserWarning, module="torch")


class DocumentProcessor:
    """
    Processes documents for ingestion into the RAG system.
    """

    def __init__(self,
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200):
        """
        Initialize document processor.

        Args:
            chunk_size: Size of text chunks for processing
            chunk_overlap: Overlap between consecutive chunks
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )

    def load_document(self, file_path: str) -> List[str]:
        """
        Load document from file path.

        Args:
            file_path: Path to the document

        Returns:
            List of document chunks
        """
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        # Select appropriate loader based on file extension
        ext = file_path.suffix.lower()

        try:
            if ext == '.pdf':
                loader = PyPDFLoader(str(file_path))
            elif ext == '.txt':
                loader = TextLoader(str(file_path))
            elif ext in ['.csv', '.tsv']:
                loader = CSVLoader(str(file_path))
            elif ext in ['.doc', '.docx']:
                loader = UnstructuredWordDocumentLoader(str(file_path))
            elif ext in ['.md', '.markdown']:
                loader = UnstructuredMarkdownLoader(str(file_path))
            else:
                raise ValueError(f"Unsupported file format: {ext}")

            documents = loader.load()
            logger.info(f"Loaded {len(documents)} documents from {file_path}")

            # Split documents into chunks
            text_chunks = self.text_splitter.split_documents(documents)
            logger.info(f"Split into {len(text_chunks)} chunks")

            # Extract text from chunks
            return [chunk.page_content for chunk in text_chunks]

        except Exception as e:
            logger.error(f"Error processing document {file_path}: {e}")
            raise

    def process_text(self, text: str) -> List[str]:
        """
        Process raw text into chunks.

        Args:
            text: Raw text to process

        Returns:
            List of text chunks
        """
        try:
            chunks = self.text_splitter.split_text(text)
            logger.info(f"Split text into {len(chunks)} chunks")
            return chunks
        except Exception as e:
            logger.error(f"Error processing text: {e}")
            raise


class KnowledgeBase:
    """
    Knowledge base for storing and retrieving document embeddings.
    """

    def __init__(self,
                 embedding_model_name: str = "sentence-transformers/all-mpnet-base-v2",
                 index_name: str = "arkillikus_index",
                 save_dir: str = "./knowledge_base"):
        """
        Initialize knowledge base.

        Args:
            embedding_model_name: Name of the embedding model
            index_name: Name of the vector index
            save_dir: Directory to save the knowledge base
        """
        self.embedding_model_name = embedding_model_name
        self.index_name = index_name
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)

        # Initialize embedding model
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
        )

        # Initialize vector store
        self.vector_store = None

        # Document processor for text chunking
        self.document_processor = DocumentProcessor()

        # Load existing index if available
        self._load_index()

    def _load_index(self):
        """Load vector index from disk if available."""
        index_path = self.save_dir / f"{self.index_name}.faiss"

        if index_path.exists():
            try:
                self.vector_store = FAISS.load_local(
                    str(self.save_dir),
                    self.embedding_model,
                    self.index_name
                )
                logger.info(f"Loaded existing index from {index_path}")
            except Exception as e:
                logger.error(f"Error loading index: {e}")
                self.vector_store = None
        else:
            logger.info(f"No existing index found at {index_path}")

    def add_documents(self, documents: List[str], metadata: List[Dict[str, Any]] = None):
        """
        Add documents to the knowledge base.

        Args:
            documents: List of document text chunks
            metadata: Optional metadata for each document
        """
        if not documents:
            logger.warning("No documents to add")
            return

        # Create vector store if doesn't exist
        if self.vector_store is None:
            logger.info("Creating new vector store")
            self.vector_store = FAISS.from_texts(
                documents,
                self.embedding_model,
                metadatas=metadata
            )
        else:
            logger.info(f"Adding {len(documents)} documents to existing vector store")
            self.vector_store.add_texts(documents, metadatas=metadata)

        # Save to disk
        self.save()

    def add_document_from_file(self, file_path: str, metadata: Dict[str, Any] = None):
        """
        Add document from file to the knowledge base.

        Args:
            file_path: Path to the document
            metadata: Optional metadata for the document
        """
        text_chunks = self.document_processor.load_document(file_path)

        # Create metadata for each chunk if provided
        chunk_metadata = None
        if metadata:
            file_name = Path(file_path).name
            chunk_metadata = [
                {**metadata, 'source': file_name, 'chunk_id': i}
                for i in range(len(text_chunks))
            ]

        self.add_documents(text_chunks, chunk_metadata)

    def add_text(self, text: str, metadata: Dict[str, Any] = None):
        """
        Add raw text to the knowledge base.

        Args:
            text: Raw text to add
            metadata: Optional metadata for the text
        """
        text_chunks = self.document_processor.process_text(text)

        # Create metadata for each chunk if provided
        chunk_metadata = None
        if metadata:
            chunk_metadata = [
                {**metadata, 'chunk_id': i}
                for i in range(len(text_chunks))
            ]

        self.add_documents(text_chunks, chunk_metadata)

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Search the knowledge base for relevant documents.

        Args:
            query: Search query
            top_k: Number of results to return

        Returns:
            List of documents with relevance scores
        """
        if self.vector_store is None:
            logger.warning("No documents in knowledge base")
            return []

        try:
            results = self.vector_store.similarity_search_with_score(query, k=top_k)

            # Format results
            documents = []
            for doc, score in results:
                # Convert score to similarity (FAISS returns distance)
                similarity = 1.0 - min(score, 1.0)

                documents.append({
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'similarity': similarity
                })

            return documents

        except Exception as e:
            logger.error(f"Error searching knowledge base: {e}")
            return []

    def save(self):
        """Save the knowledge base to disk."""
        if self.vector_store is None:
            logger.warning("No vector store to save")
            return

        try:
            self.vector_store.save_local(str(self.save_dir), self.index_name)
            logger.info(f"Saved knowledge base to {self.save_dir}")
        except Exception as e:
            logger.error(f"Error saving knowledge base: {e}")

    def get_retriever(self):
        """Get retriever for use in LangChain."""
        if self.vector_store is None:
            logger.warning("No vector store available")
            return None

        return self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}
        )

    def clear(self):
        """Clear the knowledge base."""
        self.vector_store = None

        # Remove index files
        index_path = self.save_dir / f"{self.index_name}.faiss"
        if index_path.exists():
            os.remove(index_path)

        # Remove docstore file
        docstore_path = self.save_dir / f"{self.index_name}.pkl"
        if docstore_path.exists():
            os.remove(docstore_path)

        logger.info("Knowledge base cleared")


class Arkillikus:
    """
    Main RAG-based chatbot class that combines retrieval and generation.
    """

    def __init__(self,
                 llm_model_name: str = "google/flan-t5-large",
                 embedding_model_name: str = "sentence-transformers/all-mpnet-base-v2",
                 knowledge_base: KnowledgeBase = None,
                 device: str = None,
                 load_in_8bit: bool = False,
                 verbose: bool = False):
        """
        Initialize Arkillikus chatbot.

        Args:
            llm_model_name: Name of the language model
            embedding_model_name: Name of the embedding model
            knowledge_base: Optional existing knowledge base
            device: Device to run on (defaults to CUDA if available)
            load_in_8bit: Whether to load model in 8-bit precision
            verbose: Whether to print verbose output
        """
        self.llm_model_name = llm_model_name
        self.embedding_model_name = embedding_model_name
        self.verbose = verbose

        # Set device
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")

        # Initialize knowledge base
        self.knowledge_base = knowledge_base or KnowledgeBase(
            embedding_model_name=embedding_model_name
        )

        # Initialize LLM
        self._initialize_llm(load_in_8bit)

        # Initialize RAG chain
        self._initialize_rag_chain()

        # Conversation history
        self.conversation_history = []
        self.max_history_length = 10

        logger.info("Arkillikus initialized successfully")

    def _initialize_llm(self, load_in_8bit: bool):
        """Initialize the language model and tokenizer."""
        try:
            logger.info(f"Loading LLM: {self.llm_model_name}")

            # Configure quantization if needed
            model_kwargs = {}
            if load_in_8bit and self.device == 'cuda':
                model_kwargs['quantization_config'] = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0
                )

            # Load tokenizer and model
            self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.llm_model_name,
                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,
                device_map=self.device,
                **model_kwargs
            )

            # Create text generation pipeline
            self.text_generation = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=1024,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                device=0 if self.device == 'cuda' else -1
            )

            # Create langchain LLM
            self.llm = HuggingFacePipeline(pipeline=self.text_generation)

            logger.info("LLM loaded successfully")

        except Exception as e:
            logger.error(f"Error initializing LLM: {e}")
            raise

    def _initialize_rag_chain(self):
        """Initialize the RAG chain."""
        try:
            retriever = self.knowledge_base.get_retriever()
            if retriever is None:
                logger.warning("No retriever available, RAG chain not initialized")
                self.qa_chain = None
                return

            # Create RetrievalQA chain
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True
            )

            logger.info("RAG chain initialized successfully")

        except Exception as e:
            logger.error(f"Error initializing RAG chain: {e}")
            self.qa_chain = None

    def add_document(self, file_path: str, metadata: Dict[str, Any] = None):
        """
        Add document to knowledge base.

        Args:
            file_path: Path to document
            metadata: Optional metadata for document
        """
        try:
            self.knowledge_base.add_document_from_file(file_path, metadata)
            # Re-initialize RAG chain
            self._initialize_rag_chain()

        except Exception as e:
            logger.error(f"Error adding document: {e}")
            raise

    def add_text(self, text: str, metadata: Dict[str, Any] = None):
        """
        Add text to knowledge base.

        Args:
            text: Text to add
            metadata: Optional metadata for text
        """
        try:
            self.knowledge_base.add_text(text, metadata)
            # Re-initialize RAG chain
            self._initialize_rag_chain()

        except Exception as e:
            logger.error(f"Error adding text: {e}")
            raise

    def _format_chat_history(self) -> str:
        """Format conversation history into a string."""
        if not self.conversation_history:
            return ""

        formatted = "Chat History:\n"
        for entry in self.conversation_history:
            formatted += f"User: {entry['user']}\n"
            formatted += f"Arkillikus: {entry['assistant']}\n\n"

        return formatted

    def _generate_response_with_llm(self, prompt: str) -> str:
        """Generate response using the LLM directly."""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # Decode and extract only the generated text (not the prompt)
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )

        return generated_text

    def chat(self, query: str, use_history: bool = True) -> Dict[str, Any]:
        """
        Chat with the bot.

        Args:
            query: User query
            use_history: Whether to use conversation history

        Returns:
            Dictionary with response and metadata
        """
        start_time = time.time()

        # Check if we should use conversation history
        chat_history = self._format_chat_history() if use_history else ""

        # Prepare result dictionary
        result = {
            'query': query,
            'response': None,
            'sources': [],
            'took': None
        }

        try:
            # Use RAG chain if available
            if self.qa_chain is not None:
                # Prepare input
                input_data = {
                    'query': query
                }

                # If using history, add it to the query
                if use_history and self.conversation_history:
                    input_data['query'] = f"{chat_history}\nUser: {query}\nArkillikus:"

                # Execute RAG chain
                rag_result = self.qa_chain(input_data)

                # Extract response and sources
                result['response'] = rag_result.get('result', '')
                result['sources'] = []

                # Add sources if available
                if 'source_documents' in rag_result:
                    for doc in rag_result['source_documents']:
                        result['sources'].append({
                            'content': doc.page_content,
                            'metadata': doc.metadata
                        })

            # Fall back to direct LLM if RAG chain not available
            else:
                logger.info("RAG chain not available, using direct LLM")

                # Generate prompt
                prompt = f"{chat_history}User: {query}\nArkillikus:"

                # Generate response
                generated_text = self._generate_response_with_llm(prompt)
                result['response'] = generated_text

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            result[
                'response'] = "I apologize, but I encountered an error while processing your request. Please try again."

        # Update conversation history
        self.conversation_history.append({
            'user': query,
            'assistant': result['response']
        })

        # Limit history length
        if len(self.conversation_history) > self.max_history_length:
            self.conversation_history = self.conversation_history[-self.max_history_length:]

        # Calculate time taken
        result['took'] = time.time() - start_time

        if self.verbose:
            logger.info(f"Generated response in {result['took']:.2f} seconds")
            if result['sources']:
                logger.info(f"Used {len(result['sources'])} sources")

        return result

    def search_knowledge(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Search knowledge base for relevant documents.

        Args:
            query: Search query
            top_k: Number of results to return

        Returns:
            List of relevant documents
        """
        return self.knowledge_base.search(query, top_k=top_k)

    def clear_history(self):
        """Clear conversation history."""
        self.conversation_history = []
        logger.info("Conversation history cleared")

    def clear_knowledge_base(self):
        """Clear knowledge base."""
        self.knowledge_base.clear()
        self._initialize_rag_chain()
        logger.info("Knowledge base cleared")

    def save_knowledge_base(self):
        """Save knowledge base to disk."""
        self.knowledge_base.save()

    def save_chat_history(self, file_path: str):
        """
        Save chat history to file.

        Args:
            file_path: Path to save chat history
        """
        if not self.conversation_history:
            logger.warning("No conversation history to save")
            return

        try:
            with open(file_path, 'w') as f:
                json.dump(self.conversation_history, f, indent=2)
            logger.info(f"Chat history saved to {file_path}")
        except Exception as e:
            logger.error(f"Error saving chat history: {e}")

    def load_chat_history(self, file_path: str):
        """
        Load chat history from file.

        Args:
            file_path: Path to load chat history from
        """
        try:
            with open(file_path, 'r') as f:
                self.conversation_history = json.load(f)
            logger.info(f"Loaded chat history from {file_path}")
        except Exception as e:
            logger.error(f"Error loading chat history: {e}")


# Command-line interface
def main():
    """Command-line interface for Arkillikus."""
    import argparse

    parser = argparse.ArgumentParser(description="Arkillikus: RAG-based Chatbot")
    parser.add_argument("--model", default="google/flan-t5-large", help="LLM model name")
    parser.add_argument("--embedding-model", default="sentence-transformers/all-mpnet-base-v2",
                        help="Embedding model name")
    parser.add_argument("--knowledge-dir", default="./knowledge_base", help="Knowledge base directory")
    parser.add_argument("--load-8bit", action="store_true", help="Load model in 8-bit precision")
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    parser.add_argument("--add-document", help="Add document to knowledge base")
    parser.add_argument("--interactive", action="store_true", help="Interactive chat mode")

    args = parser.parse_args()

    # Initialize knowledge base
    knowledge_base = KnowledgeBase(
        embedding_model_name=args.embedding_model,
        save_dir=args.knowledge_dir
    )

    # Initialize chatbot
    bot = Arkillikus(
        llm_model_name=args.model,
        embedding_model_name=args.embedding_model,
        knowledge_base=knowledge_base,
        load_in_8bit=args.load_8bit,
        verbose=args.verbose
    )

    # Add document if specified
    if args.add_document:
        print(f"Adding document: {args.add_document}")
        bot.add_document(args.add_document)

    # Interactive mode
    if args.interactive:
        print("Arkillikus: Hello! I'm Arkillikus, your RAG-based chatbot. How can I help you today?")
        print("Type 'exit' to quit, 'clear' to clear history, or 'help' for commands.")

        while True:
            try:
                query = input("\nYou: ")

                if query.lower() in ["exit", "quit"]:
                    print("Arkillikus: Goodbye!")
                    break

                elif query.lower() == "clear":
                    bot.clear_history()
                    print("Arkillikus: Conversation history cleared.")

                elif query.lower() == "help":
                    print("Available commands:")
                    print("  exit/quit - Exit the chat")
                    print("  clear - Clear conversation history")
                    print("  help - Show this help message")
                    print("  add <file_path> - Add document to knowledge base")
                    print("  search <query> - Search knowledge base")
                    print("  save <file_path> - Save chat history")
                    print("  load <file_path> - Load chat history")

                elif query.lower().startswith("add "):
                    file_path = query[4:].strip()
                    try:
                        bot.add_document(file_path)
                        print(f"Arkillikus: Added document: {file_path}")
                    except Exception as e:
                        print(f"Arkillikus: Error adding document: {e}")

                elif query.lower().startswith("search "):
                    search_query = query[7:].strip()
                    results = bot.search_knowledge(search_query)
                    print(f"Search results for '{search_query}':")
                    for i, result in enumerate(results):
                        print(f"Result {i + 1} (Similarity: {result['similarity']:.4f}):")
                        print(f"  Content: {result['content'][:100]}...")
                        print(f"  Metadata: {result['metadata']}")

                elif query.lower().startswith("save "):
                    file_path = query[5:].strip()
                    bot.save_chat_history(file_path)
                    print(f"Arkillikus: Chat history saved to {file_path}")

                elif query.lower().startswith("load "):
                    file_path = query[5:].strip()
                    bot.load_chat_history(file_path)
                    print(f"Arkillikus: Chat history loaded from {file_path}")

                else:
                    # Regular chat
                    result = bot.chat(query)
                    print(f"\nArkillikus: {result['response']}")

                    if result['sources'] and args.verbose:
                        print("\nSources:")
                        for i, source in enumerate(result['sources']):
                            print(f"Source {i + 1}: {source['content'][:100]}...")

                    if args.verbose:
                        print(f"Response generated in {result['took']:.2f} seconds")

            except KeyboardInterrupt:
                print("\nArkillikus: Goodbye!")
                break

            except Exception as e:
                print(f"Error: {e}")


if __name__ == "__main__":
    main()
